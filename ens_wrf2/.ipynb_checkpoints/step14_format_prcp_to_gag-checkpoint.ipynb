{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "046grids\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# covnert netcdf hourly precip to gssha gag format\n",
    "# reference: https://www.gsshawiki.com/Utility_Programs:Format_Precip_Spreadsheet\n",
    "# reference: Format Precip Macro Download (May 2009 version)\n",
    "# aggregreate hourly pcp to 3-hourly, and remove discontinous rainfall.  \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime\n",
    "from pyproj import Proj\n",
    "\n",
    "root_dir = '/glade/u/home/hongli/scratch/2019_10_01gssha/ens_forc_wrf2/scripts'\n",
    "ens_dir = os.path.join(root_dir, 'step11_downscale_daily2hr')\n",
    "gag_tpl_file = os.path.join(root_dir,'PRECIP_wwrf_2017_2018_1day_lead__no_event_24hour.gag')\n",
    "\n",
    "agg_acc_perd = 3 # firstly, accumulated precp time interval in final precp output (unit: hour)\n",
    "separate_perd = 12 # secondly, identify un-reported events that do not have percp for separate_perd continuous hours (unit: hour)\n",
    "separate_steps = separate_perd/agg_acc_perd # min row number for no precp events\n",
    "\n",
    "output_dir=os.path.join(root_dir, 'step14_format_prcp_to_gag')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "test_folders = [d for d in os.listdir(ens_dir)]\n",
    "test_folders = sorted(test_folders)\n",
    "ens_num = 100\n",
    "\n",
    "# ----------------- Coordinate configuration -----------------\n",
    "# read COORD from gag template file\n",
    "# COORD: UTM coordinates in the format easting northing.\n",
    "COORDs = []\n",
    "with open(gag_tpl_file) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i>=3 and i<=420:\n",
    "            COORDs.append(line.rstrip())\n",
    "        elif i>420:\n",
    "            break          \n",
    "\n",
    "# convert UTM coordinates to WGS84 to see these grids locations\n",
    "xllcorner = [float(x.split(' ')[1]) for x in COORDs]\n",
    "yllcorner = [float(x.split(' ')[2]) for x in COORDs]\n",
    "\n",
    "p = Proj(proj='utm',zone=10,ellps='WGS84', preserve_units=False)\n",
    "lon, lat = p(xllcorner, yllcorner, inverse=True)\n",
    "coord_gag = [[i,lat[i],lon[i]] for i in range(len(lon))]\n",
    "np.savetxt(os.path.join(output_dir,'gag_grid_coord_wgs84.txt'),coord_gag,fmt='%d,%f,%f',header='ID,longitude,latitude')\n",
    "# they are west-to-east and north-to-south. \n",
    "\n",
    "# GMET output netcdf coordinates\n",
    "ens_file = 'ens_forc.001.nc'\n",
    "f=xr.open_dataset(os.path.join(ens_dir,test_folders[0],ens_file))\n",
    "lat_gmet = f['lat'].values[:]\n",
    "lon_gmet = f['lon'].values[:]\n",
    "(ny,nx) = np.shape(lat_gmet)\n",
    "\n",
    "coord_gmet=[]\n",
    "for i in range(ny):\n",
    "    for j in range(nx):\n",
    "        coord_gmet.append([lat_gmet[i,j],lon_gmet[i,j]])\n",
    "# they are west-to-east and south-to-north. The first value of gag corresponds to netcdf pcp[-1,0]. \n",
    "# Therefore, need to flipud pcp below to write gag correctly.\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# format\n",
    "# loop through all tests\n",
    "for test_folder in test_folders[0:1]:    \n",
    "    print(test_folder)\n",
    "    \n",
    "    for m in range(ens_num):\n",
    "#     for m in range(1):\n",
    "#         print(str('%03d' % (m+1)))\n",
    "\n",
    "        mb_folder = 'mb'+ str('%03d' % (m+1))\n",
    "        if not os.path.exists(os.path.join(output_dir,test_folder)):\n",
    "            os.makedirs(os.path.join(output_dir,test_folder))\n",
    "\n",
    "        # --- part 1. read ensemble --- \n",
    "#         print('--- part 1. read ensemble --- ')\n",
    "        ens_file = 'ens_forc.'+ str('%03d' % (m+1)) +'.nc'\n",
    "        f=xr.open_dataset(os.path.join(ens_dir,test_folder,ens_file))\n",
    "        time = pd.DatetimeIndex(f['time'][:].dt.floor('H').to_pandas())\n",
    "        pcp = f.variables['Prcp'].values[:] # mm/hr        \n",
    "        \n",
    "        # --- part 2. pre-process netcdf pcp ---\n",
    "#         print('--- part 2. pre-process netcdf pcp ---')        \n",
    "        # remove the first few time steps until it starts at hours 1,4,7,10,13,16,19,22\n",
    "        # This is to avoid incomplete 3hrs data aggregation.\n",
    "        i = 0\n",
    "        start_hr_ls = [1,4,7,10,13,16,19,22]\n",
    "        while not (time[i].hour+1 in start_hr_ls): # here hour starts at 0\n",
    "            i = i+1\n",
    "        if i != 0:\n",
    "            pcp = pcp[i:,:,:]\n",
    "            time = time[i:]\n",
    "\n",
    "        # reshape pcp ( invert lat and lon dims, and 3D -> 2D)\n",
    "        pcp = np.flip(pcp, 1) # flip along axis=1 (latitude axis small-to-large -> large-to-small)\n",
    "        (nt,ny,nx) = np.shape(pcp) # (time,lat,lon)\n",
    "        pcp_new = np.reshape(pcp,(nt,ny*nx))\n",
    "\n",
    "        # define grid names for dataframe column\n",
    "        grid_names=[]\n",
    "        for j in range(ny):\n",
    "            for k in range(nx):\n",
    "                grid_names.append('row'+str(j)+'col'+str(k))\n",
    "\n",
    "        # create dataframe (time, grids)\n",
    "        df = pd.DataFrame(pcp_new,columns=grid_names)\n",
    "        df['datetime'] = time\n",
    "        df = df.set_index('datetime')\n",
    "        \n",
    "        # aggreagte precip into specified time intervals\n",
    "        df_3hr = df.resample('3H').sum() \n",
    "        time_3hr = pd.to_datetime(df_3hr.index)\n",
    "#         df_3hr.index = df_3hr.index + pd.DateOffset(hours=1) #(start at 0 -> start at 1)\n",
    "        \n",
    "        # --- part 3. separate rainfall ---\n",
    "#         print('--- part 3. separate rainfall --- ')\n",
    "        (num_time,num_grid)=np.shape(df_3hr)\n",
    "        delete_rows = [] # start row #, end row #\n",
    "        \n",
    "        i = 0\n",
    "        while (i<num_time):\n",
    "\n",
    "            if all(df_3hr.iloc[i, :]==0):\n",
    "                \n",
    "                j      = 0 # cumulative number of no-rainfall time steps\n",
    "                period = 0 # cumulative period length (hr)\n",
    "                sign   = 0 # if 1, delete these rows.\n",
    "                \n",
    "                # find all its following no-precip time steps\n",
    "                while (all(df_3hr.iloc[i, :]==0)):\n",
    "                    j = j+1\n",
    "                    period = period+j*agg_acc_perd                    \n",
    "                    i = i+1\n",
    "                    \n",
    "                    # record if no-rainfall length is beyond the separate_perd\n",
    "                    if period>=separate_perd:\n",
    "                        sign = 1\n",
    "                        \n",
    "                    # remove all the last time steps with no-rainfall (no mater length of period)\n",
    "                    # and stop loop\n",
    "                    if i == num_time-1:\n",
    "                        delete_rows.append([i-j+1,i])\n",
    "                        break                                         \n",
    "                    \n",
    "                # decide whether delete these time steps\n",
    "                if sign == 1:\n",
    "                    delete_rows.append([i-j,i]) # exlude the last, only work as index bound.\n",
    "            i = i+1\n",
    "\n",
    "        # --- part 4. write event ---\n",
    "#         print('--- part 4. write event --- ')\n",
    "        gag_ofile = 'ens_forc.'+ str('%03d' % (m+1)) +'.gag'\n",
    "        if os.path.exists(os.path.join(output_dir,test_folder,gag_ofile)):\n",
    "            os.remove(os.path.join(output_dir,test_folder,gag_ofile))                   \n",
    "\n",
    "        f_out=open(os.path.join(output_dir,test_folder,gag_ofile),'w')\n",
    "        \n",
    "        num_norain_perds = len(delete_rows)\n",
    "#         print(('event num = %d')%(num_norain_perds+1))\n",
    "        for i in range(num_norain_perds):\n",
    "            \n",
    "            EVENT = \"Event \"+str(i+1)\n",
    "            NRGAG = 418\n",
    "#             print(EVENT)\n",
    "            \n",
    "            # identify the start and end row numbers of a rainfall event\n",
    "            if i == 0:\n",
    "                start_i = 0\n",
    "                end_i = delete_rows[i][0]\n",
    "            elif i<num_norain_perds-1:\n",
    "                start_i = delete_rows[i][1]\n",
    "                end_i = delete_rows[i+1][0]\n",
    "            elif i==num_norain_perds-1:\n",
    "                if (delete_rows[i][1]-delete_rows[i][0])>=separate_steps:\n",
    "                    start_i = delete_rows[i][1]\n",
    "                    end_i = len(df_3hr)\n",
    "            NRPDS = end_i-start_i\n",
    "             \n",
    "            # write heads\n",
    "            f_out.write((\"EVENT %s\\n\") %(EVENT))\n",
    "            f_out.write((\"NRPDS %d\\n\") %(NRPDS))\n",
    "            f_out.write((\"NRGAG %d\\n\") %(NRGAG))\n",
    "            \n",
    "            # write coordinates\n",
    "            for j in range(len(COORDs)):\n",
    "                f_out.write((\"%s\\n\") %(COORDs[j]))\n",
    "            \n",
    "            # write precip\n",
    "            for j in range(start_i,end_i):\n",
    "                t=df_3hr.index[j]\n",
    "                f_out.write((\"GAGES %04d %02d %02d %02d 00 \") %(t.year,t.month,t.day,t.hour+1)) # start at 1:00, not 0:00\n",
    "                \n",
    "                data = df_3hr.iloc[j,:]\n",
    "                data_scientific = [np.format_float_scientific(x,unique=True,trim='0',precision=7) for x in data]                 \n",
    "                data_scientific =  [sub.replace('0.0e+00', '0.0') for sub in data_scientific]\n",
    "                for k in range(NRGAG):\n",
    "                    f_out.write((\"%s \") %(data_scientific[k]))\n",
    "                f_out.write(\"\\n\")\n",
    "                \n",
    "            f_out.write(\"\\n\") # end loop steps of one event                \n",
    "        f_out.close() # end loop events\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3047, 22, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime\n",
    "from pyproj import Proj\n",
    "\n",
    "root_dir = '/glade/u/home/hongli/scratch/2019_10_01gssha/ens_forc_wrf2/scripts'\n",
    "gag_tpl_file = os.path.join(root_dir,'PRECIP_wwrf_2017_2018_1day_lead__no_event_24hour.gag')\n",
    "nc_file = os.path.join(ens_dir,test_folders[0],ens_file)\n",
    "gag_ofile = \n",
    "\n",
    "agg_acc_perd = 3 # firstly, accumulated precp time interval in final precp output (unit: hour)\n",
    "separate_perd = 12 # secondly, identify un-reported events that do not have percp for separate_perd continuous hours (unit: hour)\n",
    "separate_steps = separate_perd/agg_acc_perd # min row number for no precp events\n",
    "\n",
    "# ----------------- Coordinate configuration -----------------\n",
    "# read COORD from gag template file\n",
    "# COORD: UTM coordinates in the format easting northing.\n",
    "COORDs = []\n",
    "with open(gag_tpl_file) as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i>=3 and i<=420:\n",
    "            COORDs.append(line.rstrip())\n",
    "        elif i>420:\n",
    "            break          \n",
    "\n",
    "# convert UTM coordinates to WGS84 to see these grids locations\n",
    "xllcorner = [float(x.split(' ')[1]) for x in COORDs]\n",
    "yllcorner = [float(x.split(' ')[2]) for x in COORDs]\n",
    "\n",
    "p = Proj(proj='utm',zone=10,ellps='WGS84', preserve_units=False)\n",
    "lon, lat = p(xllcorner, yllcorner, inverse=True)\n",
    "coord_gag = [[i,lat[i],lon[i]] for i in range(len(lon))]\n",
    "np.savetxt(os.path.join(output_dir,'gag_grid_coord_wgs84.txt'),coord_gag,fmt='%d,%f,%f',header='ID,longitude,latitude')\n",
    "# they are west-to-east and north-to-south. \n",
    "\n",
    "# GMET output netcdf coordinates\n",
    "f=xr.open_dataset(nc_file)\n",
    "lat_gmet = f['lat'].values[:]\n",
    "lon_gmet = f['lon'].values[:]\n",
    "(ny,nx) = np.shape(lat_gmet)\n",
    "\n",
    "coord_gmet=[]\n",
    "for i in range(ny):\n",
    "    for j in range(nx):\n",
    "        coord_gmet.append([lat_gmet[i,j],lon_gmet[i,j]])\n",
    "# they are west-to-east and south-to-north. The first value of gag corresponds to netcdf pcp[-1,0]. \n",
    "# Therefore, need to flipud pcp below to write gag correctly.\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# format\n",
    "# --- part 1. read ensemble --- \n",
    "ens_file = 'ens_forc.'+ str('%03d' % (m+1)) +'.nc'\n",
    "f=xr.open_dataset(nc_file)\n",
    "time = pd.DatetimeIndex(f['time'][:].dt.floor('H').to_pandas())\n",
    "pcp = f.variables['Prcp'].values[:] # mm/hr        \n",
    "\n",
    "# --- part 2. pre-process netcdf pcp ---\n",
    "# remove the first few time steps until it starts at hours 1,4,7,10,13,16,19,22\n",
    "# This is to avoid incomplete 3hrs data aggregation.\n",
    "i = 0\n",
    "start_hr_ls = [1,4,7,10,13,16,19,22]\n",
    "while not (time[i].hour+1 in start_hr_ls): # here hour starts at 0\n",
    "    i = i+1\n",
    "if i != 0:\n",
    "    pcp = pcp[i:,:,:]\n",
    "    time = time[i:]\n",
    "\n",
    "# reshape pcp ( invert lat and lon dims, and 3D -> 2D)\n",
    "pcp = np.flip(pcp, 1) # flip along axis=1 (latitude axis small-to-large -> large-to-small)\n",
    "(nt,ny,nx) = np.shape(pcp) # (time,lat,lon)\n",
    "pcp_new = np.reshape(pcp,(nt,ny*nx))\n",
    "\n",
    "# define grid names for dataframe column\n",
    "grid_names=[]\n",
    "for j in range(ny):\n",
    "    for k in range(nx):\n",
    "        grid_names.append('row'+str(j)+'col'+str(k))\n",
    "\n",
    "# create dataframe (time, grids)\n",
    "df = pd.DataFrame(pcp_new,columns=grid_names)\n",
    "df['datetime'] = time\n",
    "df = df.set_index('datetime')\n",
    "\n",
    "# aggreagte precip into specified time intervals\n",
    "df_3hr = df.resample('3H').sum() \n",
    "time_3hr = pd.to_datetime(df_3hr.index)\n",
    "\n",
    "# --- part 3. separate rainfall ---\n",
    "(num_time,num_grid)=np.shape(df_3hr)\n",
    "delete_rows = [] # start row #, end row #\n",
    "\n",
    "i = 0\n",
    "while (i<num_time):\n",
    "\n",
    "    if all(df_3hr.iloc[i, :]==0):\n",
    "\n",
    "        j      = 0 # cumulative number of no-rainfall time steps\n",
    "        period = 0 # cumulative period length (hr)\n",
    "        sign   = 0 # if 1, delete these rows.\n",
    "\n",
    "        # find all its following no-precip time steps\n",
    "        while (all(df_3hr.iloc[i, :]==0)):\n",
    "            j = j+1\n",
    "            period = period+j*agg_acc_perd                    \n",
    "            i = i+1\n",
    "\n",
    "            # record if no-rainfall length is beyond the separate_perd\n",
    "            if period>=separate_perd:\n",
    "                sign = 1\n",
    "\n",
    "            # remove all the last time steps with no-rainfall (no mater length of period)\n",
    "            # and stop loop\n",
    "            if i == num_time-1:\n",
    "                delete_rows.append([i-j+1,i])\n",
    "                break                                         \n",
    "\n",
    "        # decide whether delete these time steps\n",
    "        if sign == 1:\n",
    "            delete_rows.append([i-j,i]) # exlude the last, only work as index bound.\n",
    "    i = i+1\n",
    "\n",
    "# --- part 4. write event ---\n",
    "f_out=open(gag_ofile,'w')\n",
    "num_norain_perds = len(delete_rows)\n",
    "for i in range(num_norain_perds):\n",
    "\n",
    "    EVENT = \"Event \"+str(i+1)\n",
    "    NRGAG = 418\n",
    "\n",
    "    # identify the start and end row numbers of a rainfall event\n",
    "    if i == 0:\n",
    "        start_i = 0\n",
    "        end_i = delete_rows[i][0]\n",
    "    elif i<num_norain_perds-1:\n",
    "        start_i = delete_rows[i][1]\n",
    "        end_i = delete_rows[i+1][0]\n",
    "    elif i==num_norain_perds-1:\n",
    "        if (delete_rows[i][1]-delete_rows[i][0])>=separate_steps:\n",
    "            start_i = delete_rows[i][1]\n",
    "            end_i = len(df_3hr)\n",
    "    NRPDS = end_i-start_i\n",
    "\n",
    "    # write heads\n",
    "    f_out.write((\"EVENT %s\\n\") %(EVENT))\n",
    "    f_out.write((\"NRPDS %d\\n\") %(NRPDS))\n",
    "    f_out.write((\"NRGAG %d\\n\") %(NRGAG))\n",
    "\n",
    "    # write coordinates\n",
    "    for j in range(len(COORDs)):\n",
    "        f_out.write((\"%s\\n\") %(COORDs[j]))\n",
    "\n",
    "    # write precip\n",
    "    for j in range(start_i,end_i):\n",
    "        t=df_3hr.index[j]\n",
    "        f_out.write((\"GAGES %04d %02d %02d %02d 00 \") %(t.year,t.month,t.day,t.hour+1)) # start at 1:00, not 0:00\n",
    "\n",
    "        data = df_3hr.iloc[j,:]\n",
    "        data_scientific = [np.format_float_scientific(x,unique=True,trim='0',precision=7) for x in data]                 \n",
    "        data_scientific =  [sub.replace('0.0e+00', '0.0') for sub in data_scientific]\n",
    "        for k in range(NRGAG):\n",
    "            f_out.write((\"%s \") %(data_scientific[k]))\n",
    "        f_out.write(\"\\n\")\n",
    "\n",
    "    f_out.write(\"\\n\") # end loop steps of one event                \n",
    "f_out.close() # end loop events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fcfaa1816c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpcp_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3047\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "(nt,ny,nx) = np.shape(pcp)\n",
    "nt,ny,nx\n",
    "pcp_new = pcp.reshape((3047,22*19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/tools/miniconda3/envs/conda_hongli/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/conda_hongli/lib/python3.8/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mOPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"display_style\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"<pre>{escape(repr(self))}</pre>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatting_html\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/conda_hongli/lib/python3.8/site-packages/xarray/core/formatting_html.py\u001b[0m in \u001b[0;36marray_repr\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    250\u001b[0m     ]\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0msections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coords\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/conda_hongli/lib/python3.8/site-packages/xarray/core/formatting_html.py\u001b[0m in \u001b[0;36marray_section\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mdata_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"section-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mcollapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mpreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minline_variable_array_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mdata_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshort_data_repr_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mdata_icon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_icon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"icon-database\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'variable'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.Variable (time: 3047, y: 22, x: 19)>\n",
       "array([[[0.000000e+00, 6.529223e-07, ..., 0.000000e+00, 0.000000e+00],\n",
       "        [0.000000e+00, 3.712513e-05, ..., 0.000000e+00, 0.000000e+00],\n",
       "        ...,\n",
       "        [0.000000e+00, 0.000000e+00, ..., 0.000000e+00, 0.000000e+00],\n",
       "        [0.000000e+00, 0.000000e+00, ..., 0.000000e+00, 0.000000e+00]],\n",
       "\n",
       "       [[0.000000e+00, 9.793833e-07, ..., 0.000000e+00, 0.000000e+00],\n",
       "        [0.000000e+00, 5.568769e-05, ..., 0.000000e+00, 0.000000e+00],\n",
       "        ...,\n",
       "        [0.000000e+00, 0.000000e+00, ..., 0.000000e+00, 0.000000e+00],\n",
       "        [0.000000e+00, 0.000000e+00, ..., 0.000000e+00, 0.000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.000000e+00, 6.486446e-01, ..., 4.934439e-02, 0.000000e+00],\n",
       "        [0.000000e+00, 8.577239e-01, ..., 3.664310e-02, 4.787157e-02],\n",
       "        ...,\n",
       "        [0.000000e+00, 0.000000e+00, ..., 4.986373e-03, 0.000000e+00],\n",
       "        [0.000000e+00, 0.000000e+00, ..., 2.089852e-02, 0.000000e+00]],\n",
       "\n",
       "       [[0.000000e+00, 2.144245e-03, ..., 4.929714e-02, 0.000000e+00],\n",
       "        [0.000000e+00, 5.346069e-04, ..., 3.660802e-02, 5.350821e-02],\n",
       "        ...,\n",
       "        [0.000000e+00, 0.000000e+00, ..., 2.225051e-05, 0.000000e+00],\n",
       "        [0.000000e+00, 0.000000e+00, ..., 5.964157e-04, 0.000000e+00]]])\n",
       "Attributes:\n",
       "    long_name:  Precipitation\n",
       "    units:      mm/hr"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-12-01 18:00:00', '2017-12-01 21:00:00',\n",
       "               '2017-12-02 00:00:00', '2017-12-02 03:00:00',\n",
       "               '2017-12-02 06:00:00', '2017-12-02 09:00:00',\n",
       "               '2017-12-02 12:00:00', '2017-12-02 15:00:00',\n",
       "               '2017-12-02 18:00:00', '2017-12-02 21:00:00',\n",
       "               ...\n",
       "               '2018-04-07 00:00:00', '2018-04-07 03:00:00',\n",
       "               '2018-04-07 06:00:00', '2018-04-07 09:00:00',\n",
       "               '2018-04-07 12:00:00', '2018-04-07 15:00:00',\n",
       "               '2018-04-07 18:00:00', '2018-04-07 21:00:00',\n",
       "               '2018-04-08 00:00:00', '2018-04-08 03:00:00'],\n",
       "              dtype='datetime64[ns]', name='datetime', length=1020, freq='3H')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_3hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hongli",
   "language": "python",
   "name": "conda_hongli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
