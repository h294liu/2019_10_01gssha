{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read hourly WRF data\n",
      "Read daily WRF data\n",
      "Calculate portion\n",
      "Downscale\n",
      "046grids\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# This script is used to downscale daily ensemble P and T to hourly\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import datetime\n",
    "\n",
    "def read_ens(out_forc_name_base, ens_num):\n",
    "    for i in range(ens_num):\n",
    "        ens_file = os.path.join(out_forc_name_base + '.' + str('%03d' % (i+1)) +'.nc')\n",
    "        \n",
    "        f=xr.open_dataset(ens_file)\n",
    "        pcp = f.variables['pcp'][:]\n",
    "        t_mean = f.variables['t_mean'][:]\n",
    "        t_range = f.variables['t_range'][:]\n",
    "\n",
    "        if i == 0:\n",
    "            time = pd.DatetimeIndex(f['time'][:].dt.floor('D').to_pandas())\n",
    "                 \n",
    "            pcp_ens = np.zeros((np.shape(pcp)[0], np.shape(pcp)[1], np.shape(pcp)[2], ens_num))# create ens array \n",
    "            t_mean_ens = np.zeros((np.shape(pcp)[0], np.shape(pcp)[1], np.shape(pcp)[2], ens_num))\n",
    "            t_range_ens = np.zeros((np.shape(pcp)[0], np.shape(pcp)[1], np.shape(pcp)[2], ens_num))\n",
    "\n",
    "        pcp_ens[:,:,:,i] = pcp\n",
    "        t_mean_ens[:,:,:,i] = t_mean\n",
    "        t_range_ens[:,:,:,i] = t_range\n",
    "       \n",
    "    return time, pcp_ens, t_mean_ens, t_range_ens\n",
    "\n",
    "#======================================================================================================\n",
    "# main script\n",
    "root_dir='/glade/u/home/hongli/scratch/2019_10_01gssha/ens_forc_wrf2' # cheyenne\n",
    "# root_dir='/home/hongli/work/russian/ens_forc' #hydro-c1\n",
    "\n",
    "wrf_hour_file = os.path.join(root_dir,'scripts/step1_asc_to_nc/WestWRF_2017120200_2018040723.nc')\n",
    "wrf_day_file = os.path.join(root_dir,'scripts/step1_asc_to_nc/WestWRF_daily_20171202_20180407.nc')\n",
    "result_dir = os.path.join(root_dir,'test_uniform_cv')\n",
    "test_folders = [d for d in os.listdir(result_dir)]\n",
    "test_folders = sorted(test_folders)\n",
    "ens_num = 100\n",
    "\n",
    "output_dir=os.path.join(root_dir, 'scripts/step11_downscale_daily2hr')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "output_filename_base = 'ens_forc.'\n",
    "nc_tpl=os.path.join(root_dir,'scripts/step1_asc_to_nc/WestWRF_2017120200_2018040723.nc')\n",
    "    \n",
    "#======================================================================================================\n",
    "# read historical hourly wrf data\n",
    "print('Read hourly WRF data')\n",
    "f = xr.open_dataset(wrf_hour_file)\n",
    "pcp_hour = f['Prcp'].values[:] # (time, lat, lon). unit: mm/hr\n",
    "temp_hour = f['Temp'].values[:] # (time, lat, lon). unit: Fahrenheit\n",
    "(nt_hour,ny,nx)=np.shape(pcp_hour)\n",
    "\n",
    "#======================================================================================================\n",
    "# read historical daily wrf data\n",
    "print('Read daily WRF data')\n",
    "f = xr.open_dataset(wrf_day_file)\n",
    "time = pd.DatetimeIndex(f['time'][:].dt.floor('D').to_pandas())\n",
    "pcp_day = f['prcp'].values[:] # (time, lat, lon). unit: mm/day\n",
    "tmin_day = np.multiply(f['tmin'].values[:], 9.0/5.0) + 32 # unit: F. (0°C × 9/5) + 32 = 32°F.\n",
    "tmax_day = np.multiply(f['tmax'].values[:], 9.0/5.0) + 32\n",
    "(nt_day,ny,nx)=np.shape(pcp_day)\n",
    "\n",
    "#======================================================================================================\n",
    "# calcualte percent for disaggregation\n",
    "print('Calculate portion')\n",
    "# repeat daily total P and Trange into shape (nt_hour,ny,nx)\n",
    "pcp_day_repeat = np.empty_like(pcp_hour)\n",
    "tmin_day_repeat = np.empty_like(temp_hour)\n",
    "tmax_day_repeat = np.empty_like(temp_hour)\n",
    "for d in range(nt_day):\n",
    "    hr_num = 24\n",
    "    for i in range(ny):\n",
    "        for j in range(nx):\n",
    "            pcp_day_repeat[hr_num*(d):hr_num*(d+1),i,j]=pcp_day[d,i,j] \n",
    "            tmin_day_repeat[hr_num*(d):hr_num*(d+1),i,j]=tmin_day[d,i,j] \n",
    "            tmax_day_repeat[hr_num*(d):hr_num*(d+1),i,j]=tmax_day[d,i,j] \n",
    "\n",
    "# calcualte portion\n",
    "# prcp_prct = p_hour/p_day. The sum of prcp_prct on the same day is one.\n",
    "# temp_prct = (temp-tmin)/(tmax-tmin). The min and max temp_prct on the same day are zero and one, respectively.\n",
    "prcp_prct = np.divide(pcp_hour,pcp_day_repeat,\n",
    "                      out=np.ones_like(pcp_day_repeat)*(1/24.0),where=pcp_day_repeat!=0) #p/p_total\n",
    "temp_prct = np.divide(temp_hour-tmin_day_repeat,tmax_day_repeat-tmin_day_repeat,\n",
    "                     out=np.ones_like(tmin_day_repeat)*(1/24.0),where=(tmax_day_repeat-tmin_day_repeat)!=0) #t/(tmax-tmin)\n",
    "\n",
    "#======================================================================================================\n",
    "print('Downscale')\n",
    "# loop through all uniform tests\n",
    "for test_folder in test_folders[0:1]:\n",
    "    \n",
    "    print(test_folder)\n",
    "    test_dir = os.path.join(result_dir, test_folder)\n",
    "    if not os.path.exists(os.path.join(output_dir,test_folder)):\n",
    "        os.makedirs(os.path.join(output_dir,test_folder))\n",
    "\n",
    "    for i in range(ens_num):\n",
    "        \n",
    "        # read ensemble\n",
    "        ens_file = 'ens_forc.'+ str('%03d' % (i+1)) +'.nc'\n",
    "        f=xr.open_dataset(os.path.join(test_dir, 'outputs', ens_file))\n",
    "        time = pd.DatetimeIndex(f['time'][:].dt.floor('D').to_pandas())\n",
    "        pcp = f.variables['pcp'][:]\n",
    "        tmean = f.variables['t_mean'][:] # C \n",
    "        trange = f.variables['t_range'][:] \n",
    "        \n",
    "        tmin = tmean - 0.5*trange\n",
    "        tmax = tmean + 0.5*trange        \n",
    "        tmin = np.multiply(tmin, 9.0/5.0) + 32 # C to F.\n",
    "        tmax = np.multiply(tmax, 9.0/5.0) + 32\n",
    "    \n",
    "        # extend daily ensemble to hourly size\n",
    "        pcp_repeat = np.empty_like(pcp_hour)\n",
    "        tmin_repeat = np.empty_like(temp_hour)\n",
    "        tmax_repeat = np.empty_like(temp_hour)\n",
    "        hr_num = 24\n",
    "        for d in range(len(time)):\n",
    "            pcp_repeat[hr_num*d:hr_num*(d+1),:,:]=pcp[d,:,:] \n",
    "            tmin_repeat[hr_num*d:hr_num*(d+1),:,:]=tmin[d,:,:] \n",
    "            tmax_repeat[hr_num*d:hr_num*(d+1),:,:]=tmax[d,:,:]\n",
    "\n",
    "        # calculate hourly value for an ensemble member. (nt_hour,ny,nx).\n",
    "        pcp_hour_ens = np.multiply(prcp_prct, pcp_repeat) \n",
    "        temp_hour_ens = np.multiply(temp_prct, (tmax_repeat-tmin_repeat))+tmin_repeat\n",
    "        \n",
    "        # write to a netcdf file        \n",
    "        with nc.Dataset(nc_tpl) as src:\n",
    "            with nc.Dataset(os.path.join(output_dir,test_folder, ens_file), \"w\") as dst:\n",
    "                \n",
    "                # copy dimensions\n",
    "                for name, dimension in src.dimensions.items():\n",
    "                     dst.createDimension(\n",
    "                        name, (len(dimension) if not dimension.isunlimited() else None))\n",
    "        \n",
    "                # copy variable attributes all at once via dictionary (for the included variables)\n",
    "                include = ['lat', 'lon', 'time', 'Temp','Prcp']\n",
    "                for name, variable in src.variables.items():\n",
    "                    if name in include:\n",
    "                        x = dst.createVariable(name, variable.datatype, variable.dimensions)               \n",
    "                        dst[name].setncatts(src[name].__dict__)\n",
    "                        if name!='Temp' or name!='Prcp':\n",
    "                            dst[name][:]=src[name][:]                \n",
    "                \n",
    "                dst.variables['Prcp'][:] = pcp_hour_ens\n",
    "                dst.variables['Temp'][:] = temp_hour_ens    \n",
    "\n",
    "        del pcp,tmean,trange,pcp_repeat,tmin_repeat,tmax_repeat,pcp_hour_ens,temp_hour_ens \n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.68468742, 0.68468742, 0.68468742, 0.68468742, 0.68468742,\n",
       "        0.68468742, 0.68468742, 0.68468742, 0.68468742, 0.68468742,\n",
       "        0.68468742, 0.68468742, 0.68468742, 0.68468742, 0.68468742,\n",
       "        0.68468742, 0.68468742, 0.68468742, 0.68468742, 0.68468742,\n",
       "        0.68468742, 0.68468742, 0.68468742, 0.68468742]),\n",
       " 0.6846874208356961)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=10\n",
    "j=10\n",
    "d=1\n",
    "hr_num=24\n",
    "pcp_day_repeat[hr_num*(d):hr_num*(d+1),i,j],pcp_day[d,i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.19631960e-04, 1.41492800e-04, 4.55140730e-02, 9.08866500e-02,\n",
       "        1.36259240e-01, 1.21451326e-01, 1.06643416e-01, 9.18355100e-02,\n",
       "        6.12237380e-02, 3.06119640e-02, 1.89537840e-07, 1.26358570e-07,\n",
       "        6.31792860e-08, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " array([1.74724928e-04, 2.06653132e-04, 6.64742357e-02, 1.32741814e-01,\n",
       "        1.99009411e-01, 1.77382149e-01, 1.55754893e-01, 1.34127643e-01,\n",
       "        8.94185232e-02, 4.47094003e-02, 2.76823897e-07, 1.84549279e-07,\n",
       "        9.22746411e-08, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcp_hour[hr_num*(d):hr_num*(d+1),i,j],prcp_prct[hr_num*(d):hr_num*(d+1),i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.74724928e-04, 2.06653132e-04, 6.64742357e-02, 1.32741814e-01,\n",
       "       1.99009411e-01, 1.77382149e-01, 1.55754893e-01, 1.34127643e-01,\n",
       "       8.94185232e-02, 4.47094003e-02, 2.76823897e-07, 1.84549279e-07,\n",
       "       9.22746411e-08, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcp_hour[hr_num*(d):hr_num*(d+1),i,j]/pcp_day_repeat[hr_num*(d):hr_num*(d+1),i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "        0.04166667, 0.04166667, 0.04166667]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcp_hour[0:8,i,j],prcp_prct[0:8,i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17851137, 0.33333331, 0.48815532])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcp_hour[-3:,i,j]/pcp_day_repeat[-3:,i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-806a62e80e7e>:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  a=np.where(pcp_day_repeat==0,1/24.0,np.divide(pcp_hour,pcp_day_repeat))\n",
      "<ipython-input-8-806a62e80e7e>:1: RuntimeWarning: overflow encountered in true_divide\n",
      "  a=np.where(pcp_day_repeat==0,1/24.0,np.divide(pcp_hour,pcp_day_repeat))\n",
      "<ipython-input-8-806a62e80e7e>:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  a=np.where(pcp_day_repeat==0,1/24.0,np.divide(pcp_hour,pcp_day_repeat))\n"
     ]
    }
   ],
   "source": [
    "a=np.where(pcp_day_repeat==0,1/24.0,np.divide(pcp_hour,pcp_day_repeat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.16666667e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.16666667e-02, 4.16666667e-02, 4.16666667e-02,\n",
       "       0.00000000e+00, 4.16666667e-02, 4.16666667e-02, 0.00000000e+00,\n",
       "       4.16666667e-02, 4.16666667e-02, 4.16666667e-02, 4.16666667e-02,\n",
       "       8.64554878e-05, 1.11102582e-01, 2.22118707e-01, 3.33134832e-01,\n",
       "       2.22123973e-01, 1.11113100e-01, 1.02227202e-04, 1.31666878e-04])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[8:8+24,10,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_hongli",
   "language": "python",
   "name": "conda_hongli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
